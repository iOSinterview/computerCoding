## 5、一致性哈希—解决缓存雪崩与负载均衡

---

## （1）为什么使用一致性哈希？

### 1.1 我该选取谁？

对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

假设第一次**随机选取**了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。**这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。**

那有什么办法，**对于给定的 key，每一次都选择同一个节点**呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法

![hash select peer](https://s2.loli.net/2024/09/01/mYkiD2KB5JPacrQ.jpg)。

从上面的图可以看到，任意一个节点任意时刻请求查找键 `Tom` 对应的值，都会分配给节点 2，有效地解决了上述的问题。

### 1.2  节点数量变化了怎么办？

简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，**移除了其中一台节点，只剩下 9 个，那么之前 `hash(key) % 10` 变成了 `hash(key) % 9`，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 `缓存雪崩`。**

> 缓存雪崩：缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。常因为缓存服务器宕机，或缓存设置了相同的过期时间引起。

一致性哈希可以解决缓存雪崩。

## （2）算法原理

### 2.1 简单一致性哈希

一致性哈希算法将 key 映射到 $2^{32}$ 的空间中，将这个数字首尾相连，形成一个环。

- 计算**节点/机器**(通常使用**节点的名称、编号和 IP 地址**)的哈希值，放置在环上。
- 计算 key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点。

![一致性哈希添加节点 consistent hashing add peer](https://s2.loli.net/2024/09/01/iO6B87F91LIHwbD.jpg)

环上有 peer2，peer4，peer6 三个节点，`key11`，`key2`，`key27` 均映射到 peer2，`key23` 映射到 peer4。此时，如果新增节点/机器 peer8，假设它新增位置如图所示，那么只有 `key27` 从 peer2 调整到 peer8，其余的映射均没有发生改变。

也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点，这就解决了上述的问题。

### 2.2 数据倾斜问题（负载均衡）

如果服务器的节点过少，容易引起 key 的倾斜。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。**那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，缓存节点间负载不均**。

**为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。**

假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

- 第一步，计算虚拟节点的 Hash 值，放置在环上。
- 第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

**虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(`map`)维护真实节点与虚拟节点的映射关系即可。**

## （3）算法实现

新建 package `consistenthash`，用来实现一致性哈希算法。

### 3.1 结构

![consistency](https://s2.loli.net/2024/09/01/NgR3bzwvIj5lKOG.jpg)

```go
// HashFunc 定义哈希函数输入输出
type HashFunc func(data []byte) uint32

// Consistency 维护peer与其hash值的关联
type Consistency struct {
	hashFunc HashFunc       // 哈希函数依赖
	replicas int            // 虚拟节点个数(防止数据倾斜)
	hashRing []int          // uint32哈希环
	peerMap  map[int]string // hashValue -> peerName，虚拟节点：真实节点
}

func New(replicas int, fn HashFunc) *Consistency {
	c := &Consistency{
		replicas: replicas,
		hashFunc: fn,
		peerMap:  make(map[int]string),
	}
	if c.hashFunc == nil {
		c.hashFunc = crc32.ChecksumIEEE
	}
	return c
}
```

- 定义了函数类型 `HashFunc`，采取依赖注入的方式，允许用于替换成自定义的 Hash 函数，也方便测试时替换，默认为 `crc32.ChecksumIEEE` 算法。
- `Consistency` 是一致性哈希算法的主数据结构，包含 4 个成员变量：HashFunc 函数 `hashFunc`；虚拟节点倍数 `replicas`；哈希环 `hashRing`；虚拟节点与真实节点的映射表 `peerMap`，键是虚拟节点的哈希值，值是真实节点的名称。
- 构造函数 `New()` 允许自定义虚拟节点倍数和 Hash 函数。

### 3.2 注册节点

```go
// Register 将各个peer注册到哈希环上
func (c *Consistency) Register(peersName ...string) {
	for _, peerName := range peersName {
		for i := 0; i < c.replicas; i++ {
			hashValue := int(c.hashFunc([]byte(strconv.Itoa(i) + peerName)))
			c.hashRing = append(c.hashRing, hashValue)
			c.peerMap[hashValue] = peerName
		}
	}
	sort.Ints(c.hashRing)
}
```

- `Rigister`函数允许传入 0 或 多个真实节点的名称。
- 对每一个真实节点 `key`，对应创建 `c.replicas` 个虚拟节点，虚拟节点的名称是：`strconv.Itoa(i) + key`，即通过添加编号的方式区分不同虚拟节点。
- 使用 `c.hashFunc()` 计算虚拟节点的哈希值，使用 `append(c.hashRing, hashValue)` 添加到环上。
- 在 `peerMap` 中增加虚拟节点和真实节点的映射关系。
- 最后一步，环上的哈希值排序。

### 3.3 选择 Peer

```go
// GetPeer 计算key应缓存到的peer
func (c *Consistency) GetPeer(key string) string {
	if len(c.hashRing) == 0 {
		return ""
	}
	hashValue := int(c.hashFunc([]byte(key)))
	idx := sort.Search(len(c.hashRing), func(i int) bool {
		return c.hashRing[i] >= hashValue
	})
	return c.peerMap[c.hashRing[idx%len(c.ring)]]
}
```

- 第一步，计算 key 的哈希值。
- 第二步，顺时针找到第一个匹配的虚拟节点的下标 `idx`，从 `c.hashRing`中获取到对应的哈希值。如果 `idx == len(c.hashRing)`，说明应选择 `c.hashRing`，因为`c.hashRing` 是一个环状结构，所以用取余数的方式来处理这种情况。
- 第三步，通过 `peerMap` 映射得到真实的节点。

这里就不测试了...