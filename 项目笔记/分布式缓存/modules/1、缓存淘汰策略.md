## 缓存淘汰策略

---

gaocache 的缓存全部存储在内存中，内存是有限的，因此不可能无限制地添加数据。假定我们设置缓存能够使用的内存大小为 N，那么在某一个时间点，添加了某一条缓存记录之后，占用内存超过了 N，这个时候就需要从缓存中移除一条或多条数据了。那移除谁呢？我们肯定希望尽可能移除“没用”的数据，那如何判定数据“有用”还是“没用”呢？

常见的缓存淘汰策略有三种：

- FIFO（先进先出）
- LFU（最近最少频率使用）
- LRU（最近最少使用）

### FIFO(First In First Out)

先进先出，也就是淘汰缓存中最老(最早添加)的记录。FIFO 认为，最早添加的记录，其不再被使用的可能性比刚添加的可能性大。这种算法的实现也非常简单，创建一个队列，新增记录添加到队尾，每次内存不够时，淘汰队首。但是很多场景下，部分记录虽然是最早添加但也最常被访问，而不得不因为呆的时间太长而被淘汰。这类数据会被频繁地添加进缓存，又被淘汰出去，导致**缓存命中率降低。**

### LFU(Least Frequently Used)

最近最少频率使用，也就是淘汰缓存中访问频率最低的记录。LFU 认为，如果数据过去被访问多次，那么将来被访问的频率也更高。**LFU 的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰时选择访问次数最少的即可。LFU 算法的命中率是比较高的，但缺点也非常明显，维护每个记录的访问次数，对内存的消耗是很高的；**另外，如果数据的访问模式发生变化，LFU 需要较长的时间去适应，也就是说 LFU 算法受历史数据的影响（局部性）比较大。例如某个数据历史上访问次数奇高，但在某个时间点之后几乎不再被访问，但因为历史访问次数过高，而迟迟不能被淘汰。

### LRU(Least Recently Used)

最近最久未使用，相对于仅考虑时间因素的 FIFO 和仅考虑访问频率的 LFU，LRU 算法可以认为是**相对平衡的一种淘汰算法**。LRU 认为，如果数据最近被访问过，那么将来被访问的概率也会更高。LRU 算法的实现非常简单，维护一个队列，如果某条记录被访问了，则移动到队尾，那么队首则是最近最少访问的数据，淘汰该条记录即可。

## LRU算法实现

### 1、核心数据结构

![image-20240831211453310](https://s2.loli.net/2024/08/31/Tb1EjDhiKLCWv78.png)

底层的数据结构是`map+双向链表`

- 颜色部分为`map`，存储`key-value`的映射关系。
- `value`存储在`双向链表（double link list）`实现的队列里。将所有的值放到双向链表中，这样，当访问到某个值时，将其移动到队尾的复杂度是`O(1)`，在队尾新增一条记录以及删除一条记录的复杂度均为`O(1)`。

接下来我们构造一个底层的`lru`的`Cache`对象，并实现它的`New、Add、Get、Remove`方法。

![lru-cache](https://s2.loli.net/2024/08/31/4CQqv28FDcajetI.jpg)

### 代码：

`gaocache/modles/lru/lru.go`

```go
package lru

// lru 包实现了使用最近最久未使用使用算法的缓存功能
// 用于cache内存不足情况下 移除相应缓存记录
// Warning: lru包不提供并发一致机制
// TODO: 实现lru-k算法

import (
	"container/list"
)

// Lengthable 接口指明对象可以获取自身占有内存空间大小 以字节为单位
type Lengthable interface {
	Len() int
}

// Value 定义双向链表节点所存储的对象
type Value struct {
	key   string
	value Lengthable
}

// OnEliminated 当key-value被淘汰时 执行的处理函数
type OnEliminated func(key string, value Lengthable)

// Cache 是LRU算法实现的缓存
// 参考Leetcode使用哈希表+双向链表实现LRU
type Cache struct {
	capacity         int64 // Cache 最大容量(Byte)
	length           int64 // Cache 当前容量(Byte)
	hashmap          map[string]*list.Element
	doublyLinkedList *list.List // 链头表示最近使用
	callback OnEliminated
}

// New 创建指定最大容量的LRU缓存。
// 当maxBytes为0时，代表cache无内存限制，无限存放。
func New(maxBytes int64, callback OnEliminated) *Cache {
	return &Cache{
		capacity:         maxBytes,
		hashmap:          make(map[string]*list.Element),
		doublyLinkedList: list.New(),
		callback:         callback,
	}
}

// Get 从缓存获取对应key的value。
// ok 指明查询结果 false代表查无此key
func (c *Cache) Get(key string) (value Lengthable, ok bool) {
	if elem, ok := c.hashmap[key]; ok {
		c.doublyLinkedList.MoveToFront(elem) // 移动到队头
		entry := elem.Value.(*Value)
		return entry.value, true
	}
	return
}

// Add 增加key-value
func (c *Cache) Add(key string, value Lengthable) {
	kvSize := int64(len(key)) + int64(value.Len())
	// cache 容量检查，不够则需要淘汰
	for c.capacity != 0 && c.length+kvSize > c.capacity {
		c.Remove()
	}
	if elem, ok := c.hashmap[key]; ok {
		// 更新缓存key值
		c.doublyLinkedList.MoveToFront(elem)
		oldEntry := elem.Value.(*Value)
		// 先更新写入字节 再更新
		c.length += int64(value.Len()) - int64(oldEntry.value.Len())
		oldEntry.value = value
	} else {
		// 新增缓存key
		elem := c.doublyLinkedList.PushFront(&Value{key: key, value: value})
		c.hashmap[key] = elem
		c.length += kvSize
	}
}

// Remove 淘汰一枚最近最不常用缓存
func (c *Cache) Remove() {
	tailElem := c.doublyLinkedList.Back()
	if tailElem != nil {
		entry := tailElem.Value.(*Value)
		k, v := entry.key, entry.value
		delete(c.hashmap, k)                       // 移除映射
		c.doublyLinkedList.Remove(tailElem)        // 移除缓存
		c.length -= int64(len(k)) + int64(v.Len()) // 更新占用内存情况
		// 移除后的善后处理
		if c.callback != nil {
			c.callback(k, v)
		}
	}
}
```

- 为了通用性，我们允许`value`是实现了 `Lengthable` 接口的任意类型，该接口只包含了一个方法 `Len() int`，用于返回值所占用的内存大小。

### 测试：

`gaocache/modles/lru/lru_test.go`

## LFU算法实现